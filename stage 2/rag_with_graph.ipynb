{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x3LkpUztHNU"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jPIRSGz4tHNV"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.document_loaders.base import BaseLoader\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "L0nXP1aYtHNW"
   },
   "outputs": [],
   "source": [
    "# ChatGPT and Neo4J access data\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "os.environ[\"NEO4J_URI\"] = \"neo4j+s://4ce4d1e3.databases.neo4j.io\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"TJBTGgX8-66Q48YGoVMJYpWZJIT-L13r9JOk-w9xxU0\"\n",
    "os.environ[\"AURA_INSTANCEID\"] = \"4ce4d1e3\"\n",
    "os.environ[\"AURA_INSTANCENAME\"] = \"Instance01\"\n",
    "\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n",
      "page_content='<br>Как закрыть номер на ремонт в расширенном модуле обслуживания<br>=============================================================<br><br><br>Как закрыть номер на ремонт<br>---------------------------<br><br><br>1. В разделе «Управление номерами» → «Обслуживание номеров» нажмите на кнопку «Назначить ремонт»:<br><br><br>![8df1fcd8e2e76c25ab1323875825d2d1.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/8df1fcd8e2e76c25ab1323875825d2d1.png)<br>\\xa0<br><br><br>Чтобы назначить ремонт в списке номеров, нажмите на кнопку «Действие» и выберите «Назначить ремонт»:<br><br><br>![7bcefafba009e72f0a81a509ae301396.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/7bcefafba009e72f0a81a509ae301396.png)<br>\\xa0<br><br><br>2. Выберите нужный номер, укажите название работ, краткое описание, дату, время начала и окончания ремонта. Нажмите «Применить»:<br><br><br>![3b02458e2b83f8e020014b7afda25712.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/3b02458e2b83f8e020014b7afda25712.png)<br>\\xa0<br><br><br>3. Новый статус номера отобразится в списке номеров:<br><br><br>![8aae183870615ce87b371c8cd7f1745a.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/8aae183870615ce87b371c8cd7f1745a.png)<br>\\xa0<br><br><br>Как отредактировать информацию о ремонте<br>----------------------------------------<br><br><br>1. Перейдите в раздел «Управление отелем» → «Обслуживание номеров».<br><br><br>2. Найдите нужный номер нажмите на кнопку «Действие» и выберите «Редактировать»:<br><br><br>![5a3bdfb37793f9c85639c9278fd25170.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/5a3bdfb37793f9c85639c9278fd25170.png)<br>\\xa0<br><br><br>3. В открывшемся окне внесите необходимые изменения и нажмите «Применить»:<br><br><br>![217c6a12259305a514aeaf1d20b9d033.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/217c6a12259305a514aeaf1d20b9d033.png)<br>\\xa0<br><br><br>Как отменить ремонт<br>-------------------<br><br><br>1. В разделе «Обслуживание номеров», нажмите на кнопку действие и выберите «Удалить»:<br><br><br>![8e2db85b0e1aa657f5808141d3f6cafd.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/8e2db85b0e1aa657f5808141d3f6cafd.png)<br>\\xa0<br><br><br>2. Подтвердите удаление:<br><br><br>![288d26ae4f0eeb2cb95029ef7b1ff32d.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/288d26ae4f0eeb2cb95029ef7b1ff32d.png)<br>\\xa0<br><br><br>3. Ремонт номера отменен:<br><br><br>![ce85906e6e19dc5a49473add61c3546b.png](https://138018.selcdn.ru/KB_images/omnideskru/13074/281730/ce85906e6e19dc5a49473add61c3546b.png)<br>**Обратите внимание**. Вы не сможете отменить ремонт, если дата окончания ремонта уже прошла. <br><br>' metadata={'file_name': 'output_3_126.csv'}\n"
     ]
    }
   ],
   "source": [
    "# loading of TravelLine service responses from the text field of csv files in the data folder\n",
    "class CustomCSVLoader:\n",
    "    def __init__(self, directory_path: str, text_col_name: str, **kwargs):\n",
    "        self.directory_path = directory_path\n",
    "        self.text_col_name = text_col_name\n",
    "\n",
    "    def load(self) -> list[Document]:\n",
    "        documents = []\n",
    "        for file_name in os.listdir(self.directory_path):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory_path, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    reader = csv.DictReader(file, delimiter=\"\\t\")\n",
    "                    for row in reader:\n",
    "                        text = row.get(self.text_col_name, '')\n",
    "                        if text:\n",
    "                            #print (text)\n",
    "                            document = Document(page_content=text, metadata={'file_name': file_name})\n",
    "                            documents.append(document)\n",
    "        return documents\n",
    "\n",
    "directory_path = \"data\"\n",
    "text_col_name = \"text\"\n",
    "loader = CustomCSVLoader(directory_path, text_col_name)\n",
    "raw_documents = loader.load()\n",
    "print (len(raw_documents))\n",
    "#print (raw_documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGhtLTAStHNW",
    "outputId": "39f65561-6c6b-4370-a51f-1d8118591158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811\n"
     ]
    }
   ],
   "source": [
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "#documents = text_splitter.split_documents(raw_documents[:3])\n",
    "documents = text_splitter.split_documents(raw_documents[:300])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "pXf7OTGHtHNW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection IPv4Address(('4ce4d1e3.databases.neo4j.io', 7687)) (IPv4Address(('35.205.213.74', 7687)))\n",
      "Failed to write data to connection ResolvedIPv4Address(('35.205.213.74', 7687)) (IPv4Address(('35.205.213.74', 7687)))\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Potentially out of budget: 4808->700, expected price 0.5181, but you have only -0.059480 on account. Please, add some money to balance to proceed: https://vsegpt.ru/User/Money', 'code': 400}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13066/3028283318.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopenai_api_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.vsegpt.ru:6070/v1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gpt-4-0125-preview occasionally has issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mllm_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMGraphTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgraph_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_graph_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mconvert_to_graph_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGraphDocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGraphDocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m    224\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         nodes = (\n\u001b[1;32m    227\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmap_to_base_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2500\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4515\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4516\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4517\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4518\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return cast(\n\u001b[1;32m    157\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    559\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     async def agenerate_prompt(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         flattened_outputs = [\n\u001b[1;32m    423\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 results.append(\n\u001b[0;32m--> 411\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    412\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         )\n\u001b[0;32m-> 1232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Potentially out of budget: 4808->700, expected price 0.5181, but you have only -0.059480 on account. Please, add some money to balance to proceed: https://vsegpt.ru/User/Money', 'code': 400}}"
     ]
    }
   ],
   "source": [
    "# make the graph\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\",openai_api_base=\"https://api.vsegpt.ru:6070/v1\") # gpt-4-0125-preview occasionally has issues\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7f093e7cf310>\n"
     ]
    }
   ],
   "source": [
    "print (graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "referenced_widgets": [
      "8e37edd9789a4d57a7be401628e7ff7f",
      "9bac7003afd84cecb4e67a81a396ec8d"
     ]
    },
    "id": "RMZlhtDmtHNW",
    "outputId": "86efa842-3297-45d6-dab2-681bbc836b4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fb0b5e131d4db1b6bb9cb2e548f532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directly show the graph resulting from the given Cypher query\n",
    "default_cypher = \"MATCH (s)-[r:!INCLUDES]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "display(showGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1guHjU4uyEZK"
   },
   "source": [
    "## Hybrid Retrieval for RAG\n",
    "\n",
    "![retrieval](https://raw.githubusercontent.com/tomasonjo/blogs/master/graphhybrid.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GHbJPMfDtHNW"
   },
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(openai_api_base=\"https://api.vsegpt.ru:6070/v1\"),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nzfPwvvy0Yz"
   },
   "source": [
    "The vector index can then be called with the similarity_search method.\n",
    "## Graph retriever\n",
    "\n",
    "![graph](https://raw.githubusercontent.com/tomasonjo/blogs/master/neighbor.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yCMz_sRtHNW",
    "outputId": "f533f279-9a2b-48d6-830b-28d04c43550b"
   },
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "dY8huoM8tHNX"
   },
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-F9BjghzjdH"
   },
   "source": [
    "The `structured_retriever` function starts by detecting entities in the user question. Next, it iterates over the detected entities and uses a Cypher template to retrieve the neighborhood of relevant nodes. Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6fOJRPntHNX",
    "outputId": "a99ffca0-2d4d-4374-8519-c6e37c395f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Акция «Бесплатные Ночи» - ПРОДАЖА -> Форма Бронирования\n",
      "Средство Размещения - ПОДКЛЮЧЕНИЕ -> Тип Тарифа «Бесплатные Ночи»\n",
      "Средство Размещения - ПОДКЛЮЧЕНИЕ -> Личный Кабинет\n",
      "Средство Размещения - ПОДКЛЮЧЕНИЕ -> Шахматка Tl: Webpms\n",
      "Средство Размещения - ИНТЕГРАЦИЯ -> Асу\n",
      "Раздел Меню «Управление Номерами» - СОЗДАНИЕ -> Тарифы\n",
      "Free Night Offer - APPLIES_TO -> Guest\n",
      "Discount Calculation - APPLIES_TO -> Independent Rate\n",
      "Guest - UPLOAD_ICON -> Icon\n",
      "Guest - ENTER_PROMO_CODE -> Promo Code\n",
      "Тариф - СОДЕРЖИТ -> Условия Акции\n",
      "Special Offer - VISUALIZATION_SETTINGS -> Booking Module\n",
      "Special Offer - PRICE_DISPLAY_SETTINGS -> Price\n",
      "Special Offer - MOTIVATOR_SETTINGS -> Price\n",
      "Special Offer - OFFER_DURATION -> Special Offer\n",
      "Special Offer - STAY_CONDITIONS -> Special Offer\n",
      "Special Offer - PAYMENT_OPTIONS -> Payment Methods\n",
      "Special Offer - SALES_SOURCES -> Sales Channels\n",
      "Special Offer - CANCELLATION_POLICY -> Cancellation Rule\n",
      "Special Offer - SPECIAL_NIGHTS -> Nights\n",
      "Discount - APPLIES_TO -> Last Night\n",
      "Discount - SET_SIZE -> Room Rates\n",
      "Скидки/Надбавки - СКИДКА/НАДБАВКА -> Рабочие И Выходные Дни\n",
      "Скидки/Надбавки - ПРИМЕНЕНИЕ -> Групповая Операция\n",
      "Групповая Операция - ПРИМЕНЕНИЕ -> Категория\n",
      "Последняя Ночь - СКИДКА -> Скидки/Надбавки\n",
      "Длительное Проживание - СОЗДАНИЕ_АКЦИИ -> Раннее Бронирование\n",
      "Длительное Проживание - СОЗДАНИЕ_АКЦИИ -> Горящее Предложение\n",
      "Длительное Проживание - СОЗДАНИЕ_АКЦИИ -> Выгодное Предложение\n",
      "Длительное Проживание - СОЗДАНИЕ_АКЦИИ -> Черная Пятница И Киберпонедельник\n",
      "Промокод - ПРИМЕНЕНИЕ -> Тариф\n",
      "Промокод - ИСПОЛЬЗОВАНИЕ -> Гость\n",
      "Гость - ENTERED -> Email\n",
      "Promocode - ADD_TO -> Feedback-Letter\n",
      "Feedback-Letter - SENT_TO -> Guest\n",
      "Marketing - EMAIL_CAMPAIGN -> Email-Newsletter\n",
      "Feedback-Письмо - INCLUDES -> Промокод\n",
      "Текст Приветствия - SAVES_CHANGES -> Промокод\n",
      "Незавершенное Бронирование - UNFINISHED -> Гость\n",
      "Email - INCLUDES -> Promocode\n",
      "Promo Code - APPLY_TO_FORM -> Booking Form\n",
      "Booking Form - SUBMIT_BOOKING -> Website\n"
     ]
    }
   ],
   "source": [
    "print(structured_retriever(\"Какие бывают скидки?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN9c_dEozyaO"
   },
   "source": [
    "## Final retriever\n",
    "As we mentioned at the start, we'll combine the unstructured and graph retriever to create the final context that will be passed to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "iCTMp3prtHNX"
   },
   "outputs": [],
   "source": [
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZG9Q8Ohz3Hn"
   },
   "source": [
    "As we are dealing with Python, we can simply concatenate the outputs using the f-string.\n",
    "## Defining the RAG chain\n",
    "We have successfully implemented the retrieval component of the RAG. First, we will introduce the query rewriting part that allows conversational follow up questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "vu68Z79ttHNX"
   },
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language. \n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0,openai_api_base=\"https://api.vsegpt.ru:6070/v1\")\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsH90hbvz_aF"
   },
   "source": [
    "Next, we introduce a prompt that leverages the context provided by the integrated hybrid retriever to produce the response, completing the implementation of the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Dzb2jcittHNY"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise. Give as detailed instructions as possible to solve the problem.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3SeRw0L0Gy3"
   },
   "source": [
    "Finally, we can go ahead and test our hybrid RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "dtU0iMNgtHNY",
    "outputId": "bf3cf94c-f030-41b4-fb3b-356c5bff98f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Какие бывают скидки?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Скидки могут быть на последнюю ночь, на самостоятельный тариф, на рабочие и выходные дни, на все категории сразу, а также через промокоды.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Какие бывают скидки?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLjhppwj0Jz_"
   },
   "source": [
    "Let's test a follow up question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ln_Hz2obtHNY",
    "outputId": "1f510244-0406-4748-d5f7-71c3e56de9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Как автоматически отправлять feedback письма?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Для автоматической отправки feedback-писем с промокодами, следуйте инструкциям: \\n1. Перейдите в раздел \"Маркетинг\" > \"Email-Рассылка\".\\n2. Выберите тип письма \"Feedback-письмо\".\\n3. Добавьте промокод в поле \"Текст приветствия\" и сохраните изменения.\\n4. Для отправки письма о незавершенном бронировании с промокодом, выберите тип письма \"Незавершенное бронирование для гостя\" и добавьте промокод.\\n5. Следуйте инструкциям по настройке отправки писем в соответствии с вашими предпочтениями.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Как автоматически отправлять feedback письма?\",\n",
    "        \"chat_history\": [(\"Какие бывают скидки?\", \"Скидки могут быть на последнюю ночь, на самостоятельный тариф, на рабочие и выходные дни, на все категории сразу, а также через промокоды.\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvaTiHtNtHNY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8e37edd9789a4d57a7be401628e7ff7f": {
     "model_module": "yfiles-jupyter-graphs",
     "model_module_version": "^1.6.1",
     "model_name": "GraphModel",
     "state": {
      "_context_pane_mapping": [
       {
        "id": "Neighborhood",
        "title": "Neighborhood"
       },
       {
        "id": "Data",
        "title": "Data"
       },
       {
        "id": "Search",
        "title": "Search"
       },
       {
        "id": "About",
        "title": "About"
       }
      ],
      "_data_importer": "neo4j",
      "_directed": true,
      "_dom_classes": [],
      "_edges": [
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 2,
        "id": 14,
        "label": "RULED",
        "properties": {
         "label": "RULED"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 3,
        "id": 15,
        "label": "RULED",
        "properties": {
         "label": "RULED"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 4,
        "id": 16,
        "label": "BELONGED_TO",
        "properties": {
         "label": "BELONGED_TO"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 5,
        "id": 17,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 6,
        "id": 18,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 6,
        "id": 19,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 7,
        "id": 20,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#607D8B",
        "directed": true,
        "end": 8,
        "id": 21,
        "label": "BEQUEATHED_CROWN_TO",
        "properties": {
         "label": "BEQUEATHED_CROWN_TO"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 9,
        "id": 22,
        "label": "IGNORED_CLAIMS_OF",
        "properties": {
         "label": "IGNORED_CLAIMS_OF"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 1,
        "id": 23,
        "label": "IGNORED_CLAIMS_OF",
        "properties": {
         "label": "IGNORED_CLAIMS_OF"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#CDDC39",
        "directed": true,
        "end": 1,
        "id": 24,
        "label": "IMPRISONED",
        "properties": {
         "label": "IMPRISONED"
        },
        "start": 9,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 10,
        "id": 25,
        "label": "DEPENDED_ON",
        "properties": {
         "label": "DEPENDED_ON"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 11,
        "id": 26,
        "label": "CREATED_TITLE",
        "properties": {
         "label": "CREATED_TITLE"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 12,
        "id": 27,
        "label": "SUCCEEDED_BY",
        "properties": {
         "label": "SUCCEEDED_BY"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 12,
        "id": 28,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 14,
        "id": 29,
        "label": "DEPENDED_ON",
        "properties": {
         "label": "DEPENDED_ON"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 17,
        "id": 40,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 18,
        "id": 41,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 19,
        "id": 42,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 3,
        "id": 43,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 20,
        "id": 44,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 21,
        "id": 45,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 22,
        "id": 46,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 23,
        "id": 47,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#607D8B",
        "directed": true,
        "end": 24,
        "id": 48,
        "label": "DEFEAT",
        "properties": {
         "label": "DEFEAT"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 12,
        "id": 64,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 7,
        "id": 65,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 29,
        "id": 66,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 1,
        "id": 67,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 30,
        "id": 68,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 31,
        "id": 69,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 32,
        "id": 70,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 33,
        "id": 71,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#CDDC39",
        "directed": true,
        "end": 34,
        "id": 72,
        "label": "PREFERRED_SUCCESSOR",
        "properties": {
         "label": "PREFERRED_SUCCESSOR"
        },
        "start": 29,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 35,
        "id": 73,
        "label": "FAMILY_RELATION",
        "properties": {
         "label": "FAMILY_RELATION"
        },
        "start": 34,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 13,
        "id": 74,
        "label": "MARRIAGE",
        "properties": {
         "label": "MARRIAGE"
        },
        "start": 35,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 37,
        "id": 91,
        "label": "GRANDPARENT",
        "properties": {
         "label": "GRANDPARENT"
        },
        "start": 39,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 38,
        "id": 92,
        "label": "GRANDPARENT",
        "properties": {
         "label": "GRANDPARENT"
        },
        "start": 39,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 40,
        "id": 93,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 34,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 38,
        "id": 94,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 40,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 37,
        "id": 95,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 16,
        "id": 96,
        "label": "CONTENDER",
        "properties": {
         "label": "CONTENDER"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 8,
        "id": 97,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 46,
        "id": 98,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 47,
        "id": 99,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 41,
        "id": 100,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 44,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 42,
        "id": 101,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 44,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 41,
        "id": 102,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 45,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 48,
        "id": 103,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 46,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 51,
        "id": 117,
        "label": "HELD_TITLE",
        "properties": {
         "label": "HELD_TITLE"
        },
        "start": 50,
        "thickness_factor": 1
       }
      ],
      "_graph_layout": {},
      "_highlight": [],
      "_license": {},
      "_model_module": "yfiles-jupyter-graphs",
      "_model_module_version": "^1.6.1",
      "_model_name": "GraphModel",
      "_neighborhood": {},
      "_nodes": [
       {
        "color": "#2196F3",
        "id": 1,
        "label": "Elizabeth I",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth I",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#4CAF50",
        "id": 2,
        "label": "England",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "England",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 3,
        "label": "Ireland",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Ireland",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#F44336",
        "id": 4,
        "label": "House Of Tudor",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "House Of Tudor",
         "label": "__Entity__:Royal family"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#F44336"
       },
       {
        "color": "#2196F3",
        "id": 5,
        "label": "Henry Viii",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Viii",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 6,
        "label": "Anne Boleyn",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Anne Boleyn",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 7,
        "label": "Edward Vi",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Edward Vi",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 8,
        "label": "Lady Jane Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Jane Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 9,
        "label": "Mary",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 10,
        "label": "William Cecil",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "William Cecil",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#607D8B",
        "id": 11,
        "label": "Baron Burghley",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Baron Burghley",
         "label": "Title:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#607D8B"
       },
       {
        "color": "#2196F3",
        "id": 12,
        "label": "James Vi Of Scotland",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "James Vi Of Scotland",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 13,
        "label": "Mary, Queen Of Scots",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary, Queen Of Scots",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 14,
        "label": "Francis Walsingham",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Francis Walsingham",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 16,
        "label": "Elizabeth",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#4CAF50",
        "id": 17,
        "label": "Spain",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Spain",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 18,
        "label": "Netherlands",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Netherlands",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 19,
        "label": "France",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "France",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#673AB7",
        "id": 20,
        "label": "William Shakespeare",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "William Shakespeare",
         "label": "Playwright:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#673AB7"
       },
       {
        "color": "#673AB7",
        "id": 21,
        "label": "Christopher Marlowe",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Christopher Marlowe",
         "label": "Playwright:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#673AB7"
       },
       {
        "color": "#CDDC39",
        "id": 22,
        "label": "Francis Drake",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Francis Drake",
         "label": "Explorer:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#CDDC39"
       },
       {
        "color": "#CDDC39",
        "id": 23,
        "label": "Walter Raleigh",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Walter Raleigh",
         "label": "Explorer:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#CDDC39"
       },
       {
        "color": "#9E9E9E",
        "id": 24,
        "label": "Spanish Armada",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Spanish Armada",
         "label": "Event:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#9E9E9E"
       },
       {
        "color": "#2196F3",
        "id": 29,
        "label": "Mary I",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary I",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 30,
        "label": "Jane Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Jane Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 31,
        "label": "Katherine Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Katherine Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 32,
        "label": "Mary Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 33,
        "label": "Margaret Clifford",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Clifford",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 34,
        "label": "Margaret Douglas",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Douglas",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 35,
        "label": "Henry Stuart, Lord Darnley",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Stuart, Lord Darnley",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 39,
        "label": "Margaret Tudor",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Tudor",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 37,
        "label": "James Vi",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "James Vi",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 38,
        "label": "Arbella Stuart",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Arbella Stuart",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 40,
        "label": "Charles Stuart",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Charles Stuart",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 41,
        "label": "Frances Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Frances Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 46,
        "label": "Lady Catherine Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Catherine Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 47,
        "label": "Lady Mary Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Mary Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 44,
        "label": "Charles Brandon",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Charles Brandon",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 42,
        "label": "Eleanor Clifford",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Eleanor Clifford",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 45,
        "label": "Henry Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 48,
        "label": "Henry Herbert",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Herbert",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 50,
        "label": "Elizabeth Petrovna",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth Petrovna",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#607D8B",
        "id": 51,
        "label": "Empress Of Russia",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Empress Of Russia",
         "label": "Title:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#607D8B"
       }
      ],
      "_overview": {
       "enabled": null,
       "overview_set": false
      },
      "_selected_graph": [
       [],
       []
      ],
      "_sidebar": {
       "enabled": true,
       "start_with": ""
      },
      "_view_count": null,
      "_view_module": "yfiles-jupyter-graphs",
      "_view_module_version": "^1.6.1",
      "_view_name": "GraphView",
      "layout": "IPY_MODEL_9bac7003afd84cecb4e67a81a396ec8d"
     }
    },
    "9bac7003afd84cecb4e67a81a396ec8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": "800px",
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}