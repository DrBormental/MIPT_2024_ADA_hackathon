## Stage 2. LLM RAG + Graph (combines vector and keyword indexes with graph retrieval for RAG applications)

В папке представлено решение (rag_with_graph.ipynb) позволяющее создать граф знаний и использовать его вместе с LLM RAG, что увеличивает точность ответов.
Решение работает с использованием ChatGPT-3.5, Neo4j на сервере с CPU

Кроме того, рекомендуем разработчикам рассмотреть возможность использования headless чат-бот платформы RasaGPT, а так же Q&A сервиса FractalGPT 


#### Ответы на комментарии по первой части.

Наше решение построено на фреймворке Langchain для использования больших языковых моделей. 

1) Это позволяет использовать как on-premise языковые модели, так и доступ по API к множеству моделей, например, в т.ч. российским GigaChat и YandexGPT.

2) Использование на входе аудио и графических данных от клиента
Используемый в проекте фреймворк Langchain имеет возможность получать входящие данные как из текстовых файлов, так обращаться к сервисам обработки (в т.ч. локальным OCR) и извлекать текстовую информацию из аудио- и графических файлов.
см., например, https://python.langchain.com/docs/integrations/document_loaders/
Данный вариант может быть использовать в т.ч. с мессенджерами.

3) Вероятнее всего в настоящий момент база знаний хранится в PostreSQL сервере в виде отдельных статей. Они достаточно короткие и их относительно не много (тысячи - единицы тысяч документов). 
Обновление модели, построенной на этих данных занимает немного времени, таким образом можно, например, раз в сутки запускать проверку обновления статей базы знаний и в случае обнаружения таких обновлений (по дате последнего обновления), запускать обновление модели.

3) Для генерации правильного ответа в случае противоречия между статьями используем метаданные статьи (дату и время создания/обновления), даем указание LLM отдавать приоритет более свежим статьям

использовать метаданные для каждого документа (дату и время), 
после извлечения документов, подходящих для генерации ответа и в случае противоречия LLM будет опираться на самые свежие данные.

4) В случае встраивания проекта в имеющийся веб-сайт компании, веб-сервер передает в качестве параметра идентификатор сессии клиента (например, в cookie)
Это дает возможность загружать и сохранять историю диалогов с клиентом
а) получение session_id
б) запрос к базе данных (или API) - получение user_id
в) загрузка/сохранение диалогов в БД по user_id

5) Как вариант, можно рекомендовать загружать окно поддержки LLM непосредственно на странице системы управления, а не в отдельном разделе помощи, в этом случае в LLM будет передаваться название раздела и текущей страницы, что позволит LLM лучше понимать контекст запроса клиента.
В случае возникновения ошибки (через обработчик события) js-скрипт может сразу передать раздел, страницу и текст сообщения о ошибке в LLM, которая может превентивно сгенерировать и показать рекомендацию по устранению ошибки.

6) Как вариант, используя javascript библиотеку html2canvas можно создавать скриншот текущей страницы и прикреплять его к истории чата

7) Возможна загрузка чатов поддержки из мессенджеров (WhatsApp, Telegram), email и т.д. для анализа на предмет выявления типовых ошибок, анализа качества работы операторов

8) Точность ответов - 
  а) оптимизировать размер чанков
  б) использовать систему извлечения графовой структуры текста, что позволит значительно поднять точность ответов
  в) использовать языковую модель с низкой стоимостью токенов (например, GPT-3.5, которой будет достаточно для обработки)и и увеличить размер контекста
  г) использовать имеющийся список вопросов заданных клиентами для генерации ответов, а затем кэшировать ответы

9) Ввести обратную связь - оценку клиента качеством предоставленного ответа, используя ее для дальнейшего совершенствования модели.
